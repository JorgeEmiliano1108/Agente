import os
# --- IMPORTACIONES ---
from langchain_community.llms import Ollama 
from langchain_community.embeddings import OllamaEmbeddings 
from langchain_community.document_loaders import PyMuPDFLoader 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate         
from langchain_core.output_parsers import StrOutputParser     
from langchain_core.runnables import RunnablePassthrough    

print("Iniciando Agente Profe Kepler")

#CONFIGURACI√ìN
NOMBRE_PDF = "biologia.pdf"       
CARPETA_MEMORIA = "cerebro_kepler"  # Aqu√≠ se guardar√° la "memoria" del agente

#Config. IA
llm = Ollama(model="llama3", temperature=0.7)

# Cerebro para buscar informaci√≥n 
print("Cargando sistema de embeddings...")
embeddings = OllamaEmbeddings(model="nomic-embed-text") 

#Sistema de memoria inteligente 
def iniciar_vectorstore():
    """
    Esta funci√≥n verifica si ya tenemos la memoria guardada en disco.
    Si existe, la carga (r√°pido). Si no, lee el PDF y la crea (lento la primera vez).
    """
    #Verifica si ya existe CARPETA_MEMORIA
    if os.path.exists(CARPETA_MEMORIA):
        print(f"‚úÖ ¬°Memoria encontrada en '{CARPETA_MEMORIA}'! Cargando instant√°neamente...")
        # allow_dangerous_deserialization=True es necesario para cargar archivos locales propios
        return FAISS.load_local(CARPETA_MEMORIA, embeddings, allow_dangerous_deserialization=True)
    
    else:
        #Si no existe, la crea desde cero
        print("‚ö° No se encontr√≥ memoria previa. Creando nueva base de datos...")
        print(f"   -> Leyendo {NOMBRE_PDF}...")
        
        #Validaci√≥n de archivo
        if not os.path.exists(NOMBRE_PDF):
            print(f"\n[ERROR CR√çTICO] No encuentro el archivo '{NOMBRE_PDF}'.")
            print("Aseg√∫rate de que el archivo PDF est√© en la misma carpeta que este script.")
            exit()

        #Carga del PDF
        try:
            loader = PyMuPDFLoader(NOMBRE_PDF)
            documentos = loader.load()
        except Exception as e:
            print(f"[ERROR] Fall√≥ la carga del PDF: {e}")
            exit()

        if not documentos:
            print("[ERROR] El PDF parece vac√≠o o no se pudo leer texto.")
            exit()

        print(f"   -> PDF cargado. Total de p√°ginas: {len(documentos)}")

        #Divisi√≥n del texto en fragmentos
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documentos)
        
        if len(chunks) == 0:
            print("[ERROR] No se pudo extraer texto. El PDF podr√≠a ser escaneado (im√°genes).")
            exit()
            
        print(f"   -> Conocimiento dividido en {len(chunks)} fragmentos.")

        #Creaci√≥n del √≠ndice vectorial
        print("   -> Vectorizando datos (esto puede tardar unos minutos la primera vez)...")
        # Aqu√≠ usamos nomic-embed-text, que es r√°pido y no saturar√° tu RAM
        vector_store = FAISS.from_documents(chunks, embeddings)
        
        #Save in disk
        vector_store.save_local(CARPETA_MEMORIA)
        print(f"üíæ ¬°Memoria guardada exitosamente en '{CARPETA_MEMORIA}'!")
        
        return vector_store

#Inicializamos la memoria
vector_store = iniciar_vectorstore()
retriever = vector_store.as_retriever(search_kwargs={"k": 4}) 
print("Recuperador listo.")

#Definicion del prompt
system_prompt_kepler = """
Eres "Profe Kepler", un docente de Biolog√≠a y Ciencias, paciente y riguroso. 

REGLAS ABSOLUTAS:
1.  **Metodolog√≠a Socr√°tica:** Si el alumno pregunta algo b√°sico, responde con una pregunta gu√≠a.
2.  **REGLA DE ORO (RAG):** Basa tus respuestas EXCLUSIVAMENTE en el [CONTEXTO].
3.  **Citaci√≥n:** Menciona que la informaci√≥n viene de tus libros.
4.  **Si no sabes:** Di "Esa informaci√≥n no est√° en mi libro de biolog√≠a actual".
5.  **Tono:** Amable y acad√©mico.

[CONTEXTO]
{contexto}
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt_kepler),
    ("human", "{pregunta}")
])

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

#Cadena rag
rag_chain = (
    {"contexto": retriever | format_docs, "pregunta": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

print("\n" + "="*50)
print("ü§ñ PROFE KEPLER EST√Å LISTO")
print(f"üìö Libro activo: {NOMBRE_PDF}")
print(" Consejo: Si cambias el PDF, borra la carpeta 'cerebro_kepler' para regenerarla.")
print("="*50 + "\n")

#Bucle de chat
print("¬°Hola! Soy Profe Kepler. ¬øQu√© duda de Biolog√≠a tienes hoy?")
print("(Escribe 'salir' para terminar)")

while True:
    try:
        pregunta_usuario = input("\nT√∫: ")
        if pregunta_usuario.lower() in ["salir", "exit"]:
            print("¬°Adi√≥s! Sigue estudiando.")
            break
        
        if not pregunta_usuario.strip():
            continue

        print("Profe Kepler: (Consultando apuntes...)")
        respuesta = rag_chain.invoke(pregunta_usuario)
        print(f"\nProfe Kepler: {respuesta}")

    except KeyboardInterrupt:
        print("\nSesi√≥n finalizada.")
        break
    except Exception as e:
        print(f"\n[ERROR] Algo sali√≥ mal: {e}")